# -*- coding: utf-8 -*-
"""(fixed)PJ2_CF(ALS).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G-MnhoQ1oINmItT8lfkTAqROiYVu3O55
"""

from google.colab import drive
drive.mount('/content/drive')

!apt update
!apt list --upgradable
!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!tar -xvf "/content/drive/MyDrive/LDS9_DL06_K297_Hoàng Ngọc Thủy Thương/Chapter2/install/spark-3.3.0-bin-hadoop3.tgz"
!pip install -q findspark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.3.0-bin-hadoop3"

import findspark
findspark.init()

import pyspark

from pyspark import SparkContext
SparkContext.setSystemProperty('spark.hadoop.dfs.client.use.datanode.hostname', 'true')
sc = SparkContext(master="local", appName="CollaborativeFiltering_ALS_Hasaki")

sc

from pyspark.sql import SparkSession

spark = SparkSession(sc)

"""> # **Collaborative Filtering (CF) using pyspark.ml.recommendation.ALS**"""

df_ratings = spark.read.csv('/content/drive/MyDrive/LDS7_K299_Online_Hoàng Ngọc Thủy Thương/Project_2/data/danh_gia_project2_output.csv', header=True,
                      inferSchema=True)

df_ratings.show()

data_sub = df_ratings.select(['ma_san_pham', 'ma_khach_hang', 'so_sao'])
data_sub

data_sub.count()

"""## Xử lý dữ liệu"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, when, explode
from pyspark.ml.feature import StringIndexer
from pyspark.ml.recommendation import ALS
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml.evaluation import RegressionEvaluator
import time

# Lọc sản phẩm có ít nhất 5 đánh giá
product_counts = data_sub.groupBy("ma_san_pham").count().filter("count >= 5")
data_sub = data_sub.join(product_counts, "ma_san_pham", "inner")

# Lọc khách hàng có ít nhất 5 đánh giá
user_counts = data_sub.groupBy("ma_khach_hang").count().filter("count >= 5")
data_sub = data_sub.join(user_counts, "ma_khach_hang", "inner")

# Chuyển đổi `so_sao` thành dạng số
data_sub = data_sub.withColumn("so_sao", col("so_sao").cast("double"))

print(f"Số lượng bản ghi sau khi lọc: {data_sub.count()}")

"""## Chuyển đổi dữ liệu"""

# Chuyển đổi cột "so_sao" sang kiểu double
data_sub = data_sub.withColumn("so_sao", col("so_sao").cast("double"))

# Biến mã khách hàng và mã sản phẩm thành chỉ số
indexer_user = StringIndexer(inputCol="ma_khach_hang", outputCol="ma_khach_hang_idx")
indexer_product = StringIndexer(inputCol="ma_san_pham", outputCol="ma_san_pham_idx")

# Áp dụng StringIndexer
data_indexed = indexer_user.fit(data_sub).transform(data_sub)
data_indexed = indexer_product.fit(data_indexed).transform(data_indexed)

data_indexed.show(5)

# Kiểm tra xem các cột đã được thêm vào chưa
data_indexed.select('ma_khach_hang', 'ma_khach_hang_idx', 'ma_san_pham', 'ma_san_pham_idx').show(5)

# Tạo ma trận người dùng - sản phẩm (User-Item Matrix)
from pyspark.sql import functions as F

user_item_matrix = data_indexed.groupBy('ma_khach_hang_idx', 'ma_san_pham_idx') \
    .agg(F.max('so_sao').alias('rating'))

# Hiển thị một phần ma trận
user_item_matrix.show(10)

# Kiểm tra dữ liệu
training = data_indexed.select("ma_khach_hang_idx", "ma_san_pham_idx", "so_sao")
training.show(5)

# Tạo ma trận sparse (sử dụng Ma trận CSR)
def create_sparse_matrix(df, num_users, num_items):
    """
    Chuyển đổi DataFrame thành ma trận sparse dạng CSR (Compressed Sparse Row).
    """
    # Lấy các giá trị từ DataFrame
    rows = df.select('ma_khach_hang_idx', 'ma_san_pham_idx', 'rating').rdd \
        .map(lambda x: (x[0], x[1], x[2])) \
        .collect()

    # Khởi tạo một ma trận sparse rỗng
    sparse_matrix = {}

    # Lặp qua các giá trị trong rows và điền vào ma trận sparse
    for row in rows:
        user_idx = row[0]
        item_idx = row[1]
        rating = row[2]

        if user_idx not in sparse_matrix:
            sparse_matrix[user_idx] = {}
        sparse_matrix[user_idx][item_idx] = rating

    return sparse_matrix

# Tạo ma trận sparse từ dữ liệu
num_users = data_indexed.select('ma_khach_hang_idx').distinct().count()
num_items = data_indexed.select('ma_san_pham_idx').distinct().count()

sparse_matrix = create_sparse_matrix(user_item_matrix, num_users, num_items)

# Kiểm tra ma trận sparse
print("Ma trận sparse được tạo:", sparse_matrix)

"""## Tạo mô hình ALS"""

from pyspark.sql.functions import col

# Loại bỏ các bản ghi có giá trị null trong các cột "ma_khach_hang_idx", "ma_san_pham_idx", và "so_sao"
data_cleaned = data_indexed.filter(
    col("ma_khach_hang_idx").isNotNull() &
    col("ma_san_pham_idx").isNotNull() &
    col("so_sao").isNotNull()
)

# Chia dữ liệu thành tập huấn luyện và kiểm tra
(training_data, test_data) = data_cleaned.randomSplit([0.8, 0.2], seed=42)

# Lưu dữ liệu thành file CSV để sử dụng với Pandas
training_data.toPandas().to_csv("/content/drive/MyDrive/LDS7_K299_Online_Hoàng Ngọc Thủy Thương/Project_2/data/training_data.csv", index=False)
test_data.toPandas().to_csv("/content/drive/MyDrive/LDS7_K299_Online_Hoàng Ngọc Thủy Thương/Project_2/data/test_data.csv", index=False)

from pyspark.ml.recommendation import ALS

# Tạo mô hình ALS với các tham số cố định
als = ALS(
    maxIter=10,
    regParam=0.1,
    rank=20,
    userCol="ma_khach_hang_idx",
    itemCol="ma_san_pham_idx",
    ratingCol="so_sao",
    coldStartStrategy="drop"  # Bỏ các giá trị null trong dự đoán
)

# Huấn luyện mô hình
start_time = time.time()
model = als.fit(training_data)
end_time = time.time()
print(f"Thời gian huấn luyện: {end_time - start_time:.2f} giây")

# # Tạo các tham số cần tối ưu
# param_grid = (ParamGridBuilder()
#              .addGrid(als.rank, [10, 50, 100])
#              .addGrid(als.maxIter, [5, 10])
#              .addGrid(als.regParam, [0.01, 0.1, 1.0])
#              .build())

# Đánh giá mô hình bằng RMSE
evaluator = RegressionEvaluator(metricName="rmse", labelCol="so_sao", predictionCol="prediction")

"""## Cross-validation"""

# crossval = CrossValidator(
#     estimator=als,
#     estimatorParamMaps=param_grid,
#     evaluator=evaluator,
#     numFolds=3
# )

# # Kiểm tra giá trị null trong các cột
# data_indexed.filter(data_indexed["ma_khach_hang_idx"].isNull()).show()
# data_indexed.filter(data_indexed["ma_san_pham_idx"].isNull()).show()
# data_indexed.filter(data_indexed["so_sao"].isNull()).show()

# # Loại bỏ các giá trị null trong các cột quan trọng
# data_indexed = data_indexed.dropna(subset=["ma_khach_hang_idx", "ma_san_pham_idx", "so_sao"])

# # Bắt đầu đếm thời gian huấn luyện
# start_time = time.time()

# # Thực hiện cross-validation và chọn mô hình tốt nhất
# cv_model = crossval.fit(training)

# # Kết thúc thời gian huấn luyện
# end_time = time.time()
# training_time = end_time - start_time
# print(f"Thời gian huấn luyện mô hình với tuning: {training_time:.2f} giây")

# # Lấy mô hình tốt nhất
# best_model = cv_model.bestModel
# print("Bộ tham số tốt nhất:")
# print(f"Rank: {best_model._java_obj.parent().getRank()}")
# print(f"Max Iterations: {best_model._java_obj.parent().getMaxIter()}")
# print(f"RegParam: {best_model._java_obj.parent().getRegParam()}")

"""## Đánh giá mô hình"""

# Dự đoán trên tập kiểm tra
test_predictions = model.transform(test_data)

# Lọc các bản ghi có giá trị null trong cột prediction và so_sao
test_predictions = test_predictions.filter(
    test_predictions.prediction.isNotNull() & test_predictions.so_sao.isNotNull()
)

import time
from pyspark.ml.evaluation import RegressionEvaluator

# Bắt đầu đo thời gian
start_time = time.time()

# Đánh giá mô hình trên tập kiểm tra
evaluator = RegressionEvaluator(metricName="rmse", labelCol="so_sao", predictionCol="prediction")
test_rmse = evaluator.evaluate(test_predictions)

# Tính toán thời gian thực hiện
end_time = time.time()
execution_time = end_time - start_time

# In kết quả
print(f"RMSE trên tập kiểm tra: {test_rmse:.2f}")
print(f"Thời gian dự đoán: {execution_time:.4f} giây")

"""## Gợi ý sản phẩm cho khách hàng"""

from pyspark.sql.functions import col, explode, when, lit

# Đọc dữ liệu sản phẩm và khách hàng
san_pham_df = spark.read.csv('/content/drive/MyDrive/LDS7_K299_Online_Hoàng Ngọc Thủy Thương/Project_2/data/san_pham_output.csv', header=True, inferSchema=True)
khach_hang_df = spark.read.csv('/content/drive/MyDrive/LDS7_K299_Online_Hoàng Ngọc Thủy Thương/Project_2/data/Khach_hang.csv', header=True, inferSchema=True)

# from pyspark.sql.functions import col, explode, when

# def get_recommendations_for_customer(ma_khach_hang, data_indexed, model, num_recommendations=10):
#     """
#     Hàm gợi ý sản phẩm cho khách hàng dựa trên mã khách hàng.

#     Parameters:
#     - ma_khach_hang: Mã khách hàng cần dự đoán (string).
#     - data_indexed: DataFrame đã xử lý, bao gồm mã khách hàng và mã sản phẩm đã số hóa.
#     - model: Mô hình ALS đã huấn luyện.
#     - num_recommendations: Số lượng sản phẩm gợi ý (mặc định: 10).

#     Returns:
#     - DataFrame chứa mã khách hàng, mã sản phẩm và điểm dự đoán.
#     """
#     # Bước 1: Lấy chỉ số (index) của mã khách hàng từ bảng dữ liệu
#     customer_idx = data_indexed.filter(
#         data_indexed.ma_khach_hang == ma_khach_hang
#     ).select("ma_khach_hang_idx").distinct().collect()

#     if len(customer_idx) == 0:
#         print(f"Mã khách hàng {ma_khach_hang} không tồn tại trong dữ liệu!")
#         return None

#     # Lấy giá trị chỉ số (index) của khách hàng
#     ma_khach_hang_idx = customer_idx[0]["ma_khach_hang_idx"]

#     # Bước 2: Dự đoán sản phẩm gợi ý cho khách hàng
#     user_recommendations = model.recommendForUserSubset(
#         data_indexed.filter(data_indexed.ma_khach_hang_idx == ma_khach_hang_idx),
#         num_recommendations
#     )

#     # Bước 3: Nổ danh sách gợi ý thành từng dòng
#     recommendations_df = user_recommendations.select(
#         col("ma_khach_hang_idx"),
#         explode(col("recommendations")).alias("recommendation")
#     ).select(
#         col("ma_khach_hang_idx"),
#         col("recommendation.ma_san_pham_idx").alias("ma_san_pham_idx"),
#         col("recommendation.rating").alias("EstimateScore")
#     )

#     # Bước 4: Ánh xạ mã gốc để hiển thị
#     recommendations_with_original_ids = recommendations_df.join(
#         data_indexed.select("ma_khach_hang_idx", "ma_khach_hang").distinct(),
#         on="ma_khach_hang_idx"
#     ).join(
#         data_indexed.select("ma_san_pham_idx", "ma_san_pham").distinct(),
#         on="ma_san_pham_idx"
#     )

#     # Bước 5: Giới hạn EstimateScore không vượt quá 5
#     recommendations_with_original_ids = recommendations_with_original_ids.withColumn(
#         "EstimateScore",
#         when(col("EstimateScore") > 5, 5).otherwise(col("EstimateScore"))
#     )

#     # Bước 6: Lọc và hiển thị kết quả
#     result = recommendations_with_original_ids.select(
#         col("ma_khach_hang").alias("Mã khách hàng"),
#         col("ma_san_pham").alias("Mã sản phẩm"),
#         col("EstimateScore").alias("Điểm dự đoán")
#     )

#     return result

from pyspark.sql.functions import col, explode, when

# File paths
san_pham_file = "/content/drive/MyDrive/LDS7_K299_Online_Hoàng Ngọc Thủy Thương/Project_2/data/san_pham_output.csv"
khach_hang_file = '/content/drive/MyDrive/LDS7_K299_Online_Hoàng Ngọc Thủy Thương/Project_2/data/Khach_hang.csv'

def get_rated_and_recommended_products(
    ma_khach_hang, data_indexed, model, product_file, customer_file, num_recommendations=10
):
    """
    Hiển thị danh sách sản phẩm đã đánh giá và gợi ý sản phẩm với thông tin chi tiết cho khách hàng.

    Parameters:
    - ma_khach_hang: Mã khách hàng cần dự đoán (string).
    - data_indexed: DataFrame đã xử lý, bao gồm mã khách hàng và mã sản phẩm đã số hóa.
    - model: Mô hình ALS đã huấn luyện.
    - product_file: Đường dẫn đến file sản phẩm (CSV).
    - customer_file: Đường dẫn đến file khách hàng (CSV).
    - num_recommendations: Số lượng sản phẩm gợi ý (mặc định: 10).

    Returns:
    - Tuple gồm 2 DataFrames:
      1. Bảng sản phẩm đã đánh giá (Mã sản phẩm, Tên sản phẩm, Điểm đánh giá).
      2. Bảng sản phẩm được gợi ý (Mã sản phẩm, Tên sản phẩm, Điểm dự đoán).
    """
    # Load product and customer datasets
    product_df = spark.read.csv(product_file, header=True, inferSchema=True)
    customer_df = spark.read.csv(customer_file, header=True, inferSchema=True)

    # Get customer index
    customer_idx = data_indexed.filter(
        data_indexed.ma_khach_hang == ma_khach_hang
    ).select("ma_khach_hang_idx").distinct().collect()

    if len(customer_idx) == 0:
        print(f"Mã khách hàng {ma_khach_hang} không tồn tại trong dữ liệu!")
        return None, None

    ma_khach_hang_idx = customer_idx[0]["ma_khach_hang_idx"]

    # Fetch rated products
    rated_products = data_indexed.filter(
        data_indexed.ma_khach_hang_idx == ma_khach_hang_idx
    ).select("ma_san_pham", "so_sao").join(
        product_df, on="ma_san_pham", how="left"
    ).select(
        col("ma_san_pham").alias("Mã sản phẩm"),
        col("ten_san_pham").alias("Tên sản phẩm"),
        col("so_sao").alias("Điểm đánh giá")
    )

    # Generate recommendations
    user_recommendations = model.recommendForUserSubset(
        data_indexed.filter(data_indexed.ma_khach_hang_idx == ma_khach_hang_idx),
        num_recommendations
    )

    recommendations_df = user_recommendations.select(
        col("ma_khach_hang_idx"),
        explode(col("recommendations")).alias("recommendation")
    ).select(
        col("ma_khach_hang_idx"),
        col("recommendation.ma_san_pham_idx").alias("ma_san_pham_idx"),
        col("recommendation.rating").alias("EstimateScore")
    )

    recommendations_with_original_ids = recommendations_df.join(
        data_indexed.select("ma_khach_hang_idx", "ma_khach_hang").distinct(),
        on="ma_khach_hang_idx"
    ).join(
        data_indexed.select("ma_san_pham_idx", "ma_san_pham").distinct(),
        on="ma_san_pham_idx"
    )

    recommendations_with_original_ids = recommendations_with_original_ids.withColumn(
        "EstimateScore",
        when(col("EstimateScore") > 5, 5).otherwise(col("EstimateScore"))
    )

    # Enrich recommendations with product details
    recommended_products = recommendations_with_original_ids.join(
        product_df,
        on="ma_san_pham",
        how="left"
    ).select(
        col("ma_san_pham").alias("Mã sản phẩm"),
        col("ten_san_pham").alias("Tên sản phẩm"),
        col("EstimateScore").alias("Điểm dự đoán")
    )

    return rated_products, recommended_products

ma_khach_hang = "567"

# Get rated and recommended products
rated_products, recommended_products = get_rated_and_recommended_products(
    ma_khach_hang, data_indexed, model, san_pham_file, khach_hang_file, num_recommendations=10
)

# Display the results
print("Danh sách sản phẩm đã đánh giá:")
rated_products.show(truncate=False)

print("Danh sách sản phẩm được gợi ý:")
recommended_products.show(truncate=False)











